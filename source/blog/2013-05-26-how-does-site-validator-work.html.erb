---
date: 2013-05-26
title: "How does Site Validator work?"
tags: servers,css-validation,html-validation,background-queue,w3c-validation
---

<div>At Site Validator, we try to give you the most comprehensive HTML and CSS validation reports for your sites, while keeping our tool as simple as possible.</div><div><br></div><div>To validate a site, you just need to enter its main URL and click the "validate site" button. In a few seconds you start receiving results, which complete in just a few minutes.</div><div><br></div><div>But, what happens in our servers after you click the "validate site" button? Let's review our internal processes:</div><div><br></div><div>First, we normalize the site URL, and resolve its redirections to get the final URL and status. For example, you might have typed <a rel="nofollow" target="_blank" href="http://example.com">http://example.com</a> but the final URL after following redirections might be <a rel="nofollow" target="_blank" href="https://www.example.com/">https://www.example.com/</a> - we keep this final address as the good one, which will be used in the rest of the process.</div><div><br></div><div>After your site's final URL is discovered, the scraping process starts. Your main URL is visited by our web crawler, which reads the links found in it, and for all the internal links discovered, adds them to the web page processing queue.</div><div><br></div><div>Again, for each internal link we normalize the URL and resolve their redirections to get the final URL. If they're still within the main site, and they don't return a 4xx or 5xx type of error, they're added to your sitemap.</div><div><br></div><div>At this point, several background workers work in parallel to process the web pages found. Each one of them will be sent to the validation queue and also to the scraping queue.</div><div><br></div><div>Pages on the validation queue will be validated for HTML and CSS markup conformance with the W3C standards. To do this, we have our own servers with the official validation software released as open source by the W3C. We maintain separate servers, for HTML validation and for CSS validation, hosted on the great cloud service <a rel="nofollow" target="_blank" href="https://www.digitalocean.com/?refcode=55fd4e532426">Digital Ocean</a>. This allows us to scale as needed adding additional servers, as well as updating the software when a new version is available.</div><div><br></div><div>We store the validation results for each page: the number of HTML and CSS errors and warnings, as well as the specific errors found and the line where they appear on the source of your web pages. We also group them together so we can return you summary reports for your sites.</div><div><br></div><div>Each web page found will be visited again by our web crawler, to search more internal links that are within the main URL of the sitemap. They'll get added to the web page processing queue, so they'll be normalized, resolved, validated and, recursively, visited to search more internal links, repeating this process until we can't find more web pages on your site or we reach the predefined limit.</div><div><br></div><div>For each web page found we store also the response status, and the ID of the "parent" page, the one that we found the link from. This way, we can notify you of 404 and 500 errors on your pages, and show you where the link was found so you can locate and fix it.</div><div><br></div><div>Another important part of our tool is exceptions and retries. There are several points of possible temporary problems: there can be timeouts, network connectivity problems, overload... To deal with this, we've got a retrying mechanism that will retry every validation several times in the case of temporary failures. If they keep failing after that, the exception is stored so we can further investigate its cause and improve our tool.</div><div><br></div><div>That's the complexity hidden behind a single click in a button!</div>
